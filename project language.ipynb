{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b0652f8-3943-48a9-9f2e-eb5c5616c12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\diya\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\diya\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\diya\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\diya\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 624.3/624.3 kB 11.7 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f258096b-4f40-40db-9a4a-d9edf130112e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Diya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6644b81c-07d7-43f5-81b7-23367c872455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: The go to school everyday.\n"
     ]
    }
   ],
   "source": [
    "#from ... import → This means you're using a tool from a package.\n",
    "#textblob → A simple Python library that helps with language tasks like correcting \n",
    "#spelling or finding the meaning of words.\n",
    "#TextBlob → A class inside the TextBlob library that does the actual work.\n",
    "from textblob import TextBlob\n",
    "text = \"She go to scool everydy.\"\n",
    "blob = TextBlob(text)\n",
    "corrected = blob.correct()\n",
    "print(\"Corrected Sentence:\", corrected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c56b83-ed62-4de7-9769-53f8012cf51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.8-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting cmudict (from textstat)\n",
      "  Downloading cmudict-1.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\diya\\anaconda3\\lib\\site-packages (from textstat) (75.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=5 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from cmudict->textstat) (7.0.1)\n",
      "Collecting importlib-resources>=5 (from cmudict->textstat)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from importlib-metadata>=5->cmudict->textstat) (3.17.0)\n",
      "Downloading textstat-0.7.8-py3-none-any.whl (239 kB)\n",
      "Downloading cmudict-1.1.1-py3-none-any.whl (939 kB)\n",
      "   ---------------------------------------- 0.0/939.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 939.7/939.7 kB 10.9 MB/s eta 0:00:00\n",
      "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 19.5 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: pyphen, importlib-resources, cmudict, textstat\n",
      "Successfully installed cmudict-1.1.1 importlib-resources-6.5.2 pyphen-0.17.2 textstat-0.7.8\n"
     ]
    }
   ],
   "source": [
    "#We'll use a library called textstat, which gives scores like:\n",
    "#Flesch Reading Ease – higher score = easier to read\n",
    "#Gunning Fog Index – shows the years of education needed to understand the text\n",
    "!pip install textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d145b13-bc98-4eb6-852b-c5dc58ac509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flesch Reading Ease Score: 83.32000000000004\n",
      "Gunning Fog Index: 2.0\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "def readability(text):\n",
    "    flesch = textstat.flesch_reading_ease(text)\n",
    "    fog = textstat.gunning_fog(text)\n",
    "    \n",
    "    print(\"Flesch Reading Ease Score:\", flesch)  #Flesch Reading Ease Score: 83.32\n",
    "                                                #This means your sentence is very easy to read.\n",
    "                                                #Around 5th-grade level.\n",
    "    \n",
    "    print(\"Gunning Fog Index:\", fog)     #A Fog Index of 2.0 means someone with just 2 years of education\n",
    "                                         #could understand it.\n",
    "\n",
    "# Try with corrected sentence\n",
    "readability(\"The go to school everyday.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae1fffd2-6eab-4d09-8d97-22c8759cd8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\diya\\anaconda3\\lib\\site-packages (4.54.0)\n",
      "Requirement already satisfied: torch in c:\\users\\diya\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\diya\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from transformers) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\diya\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\diya\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\diya\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\diya\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\diya\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\diya\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "#transformers → for pre-trained language models\n",
    "#torch → for running deep learning models\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f5100a2-8c88-4c0b-b1e6-d45981ffef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tone: surprise\n",
      "Confidence: 0.37\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the emotion detection model\n",
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", \n",
    "                      top_k=1) #This tells Python: \"I want a tool that can classify text into emotions.\n",
    "                               #top_k=1 This shows only the strongest emotion with the highest confidence.\n",
    "# Define a function to detect emotion\n",
    "def detect_tone(text):\n",
    "    result = classifier(text)\n",
    "    label = result[0][0]['label']\n",
    "    confidence = round(result[0][0]['score'], 2)\n",
    "    print(\"Tone:\", label)\n",
    "    print(\"Confidence:\", confidence)\n",
    "\n",
    "# Try with sentence\n",
    "detect_tone(\"I can't believe I failed again!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce85f4dc-c8a1-4252-aa77-af176083fc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 981.5/981.5 kB 15.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\diya\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993251 sha256=ef9dcf6cdb68f61216eed4e74018c6565d2a3268286e2816e90478439f119461\n",
      "  Stored in directory: c:\\users\\diya\\appdata\\local\\pip\\cache\\wheels\\c1\\67\\88\\e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ad7a2d8-b429-41b0-970e-0d373120951c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langcodes\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\diya\\anaconda3\\lib\\site-packages (from marisa-trie>=1.1.0->language-data>=1.2->langcodes) (75.1.0)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.4/5.4 MB 32.8 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Installing collected packages: marisa-trie, language-data, langcodes\n",
      "Successfully installed langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langcodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6dc566e-6b02-4ea8-85d4-9b7ad63bbe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: English (en)\n",
      "Language: Hindi (hi)\n",
      "Language: French (fr)\n"
     ]
    }
   ],
   "source": [
    "from langcodes import Language\n",
    "def detect_language_name(text):\n",
    "    code = detect(text)\n",
    "    name = Language.make(language=code).display_name()\n",
    "    print(f\"Language: {name} ({code})\")\n",
    "# Example\n",
    "detect_language_name(\"She goes to school every day.\")\n",
    "detect_language_name(\"वह हर दिन स्कूल जाती है।\")\n",
    "detect_language_name(\"Elle va à l'école tous les jours.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e41f8-9fa7-48cd-b54d-fb9071810236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
